
flash attention illustration

<img src="./assets/From Softmax to FlashAttention Illustration.png" alt="flash" width="500"/>

As MHA can be parallelized by each head, we simply implement one head for the sake of simplicity.

